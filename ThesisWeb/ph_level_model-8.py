# -*- coding: utf-8 -*-
"""pH level model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-3GvQKCubmUwucrIgvG9-1kvIsv_nhLM
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, make_scorer

data = pd.read_csv('ph_data.csv')

data

#selecting X and y for classification.
X = data.iloc[:, 3:].values
y = data.iloc[:, 2].values

print(X)

'''

plt.figure(figsize=(10, 6))

# Scatter plot for red color
plt.scatter(data['pH Level'], data['Red'], color='Red', label='Red', alpha=0.7)

# Scatter plot for green color
plt.scatter(data['pH Level'], data['Green'], color='Green', label='Green', alpha=0.7)

# Scatter plot for blue color
plt.scatter(data['pH Level'], data['Blue'], color='Blue', label='Blue', alpha=0.7)

# Set labels and title
plt.xlabel('pH Level')
plt.ylabel('Color Value')
plt.title('RGB Colors vs pH Level')

# Add legend
plt.legend()

# Show the plot
plt.show()
'''
#reshaping y for regression.
y = y.reshape(len(y),1)
#print(y)

from sklearn.preprocessing import StandardScaler
# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

#splitting the dataset into test and training.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, random_state = 1)


print(X_test)

'''
# List of features to compare with 'Profit'
features = ['Red', 'Green', 'Blue']

# Create scatter plots for each feature vs. 'Profit'
for feature in features:
    plt.figure(figsize=(8, 6))
    plt.scatter(data[feature], data['pH Level'])
    plt.title(f'Scatter Plot: {feature} vs. pH')
    plt.xlabel(feature)
    plt.ylabel('label')
    plt.show()
'''
rf_model = RandomForestRegressor(n_estimators=3, random_state=10)
rf_model.fit(X_train, y_train)

from sklearn.metrics import r2_score

# Assuming you've already trained your rf_model
y_pred = rf_model.predict(X_test)

# Calculate the R2 score
r2 = r2_score(y_test, y_pred)

# Print the R2 score
print(f'R-squared (R2) Score: {r2}')

#dataset is consistent.
ds_plot = data.pH.value_counts()
ds_plot.plot(kind = 'bar')
plt.xticks(rotation=45)
plt.xlabel('pH Scale')
plt.ylabel('Count')
plt.savefig("count_plot.png", dpi = 1000, transparent = True)

# Visualize a single decision tree from the Random Forest (e.g., the first tree)
from sklearn.tree import plot_tree

plt.figure(figsize=(20, 10))
plot_tree(rf_model.estimators_[0], feature_names=['Red', 'Green', 'Blue'], filled=True, rounded=True)
plt.savefig("decision_tree.png", dpi=300, bbox_inches='tight')
plt.show()

#pip install scikit-learn tensorflow
'''

import joblib
joblib.dump(rf_model,'pH_model-v6.pkl')

import joblib

# Assuming 'model' is your scikit-learn model
joblib.dump(model, 'sklearn_model.pkl')

import numpy as np

# Assuming you've already trained your rf_model
y_pred = rf_model.predict(X_test)

# Assuming 'y_test' and 'y_pred' are your actual and predicted values
# Stack arrays horizontally
comparison_array = np.column_stack((y_test, y_pred))

# Print or display the side-by-side comparison
print("Side-by-side comparison of actual and predicted:")
print(comparison_array)

# Import necessary libraries
from sklearn.tree import plot_tree

# Visualize a single decision tree from the Random Forest (e.g., the first tree)
plt.figure(figsize=(20, 10))
plot_tree(rf_model.estimators_[0], feature_names=['red', 'green', 'blue'], filled=True, rounded=True)
plt.show()

'''